# Qwen æ¨¡å‹ä»£ç å¤ç”¨é‡æ„æŠ¥å‘Š

## ä¸€ã€é‡æ„èƒŒæ™¯ä¸åŠ¨æœº

é‡æ„å‰ï¼Œ`Qwen2Model`ï¼ˆ2104è¡Œï¼‰ã€`Qwen3Model`ï¼ˆ2095è¡Œï¼‰ã€`Qwen3VLModel`ï¼ˆ3088è¡Œï¼‰ä¸‰ä¸ªæ¨¡å‹æ–‡ä»¶å„è‡ªç‹¬ç«‹ç»§æ‰¿ `Model` åŸºç±»ï¼Œå­˜åœ¨**çº¦ 1800 è¡Œå‡ ä¹å®Œå…¨ç›¸åŒçš„ä»£ç **ã€‚

é€šè¿‡é€ä¸€å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„æ–¹æ³•å®ç°ï¼Œå‘ç°å·®å¼‚é›†ä¸­åœ¨æå°‘æ•°åœ°æ–¹ï¼š

| æ–¹æ³• | Qwen2 vs Qwen3 å·®å¼‚ |
|------|---------------------|
| `forward`, `predict`, `embedding`, `cls_logits`, `post_processing` | **å®Œå…¨ç›¸åŒ** |
| `attention_rms`, `attention_mha`, `attention_mha_with_graph` | **å®Œå…¨ç›¸åŒ** |
| `feed_forward`, `feed_forward_fused` | Qwen3 å¤šäº† AWQ åˆ†æ”¯åˆ¤æ–­ï¼ŒQwen2 æ²¡æœ‰ â†’ ç”¨è¶…é›†æ–¹æ¡ˆç»Ÿä¸€ |
| `batched_*`ï¼ˆ6ä¸ªæ–¹æ³•ï¼‰, `prefill`, `decode`, `clear_kv_cache` | åŒä¸Šï¼ŒQwen3 å¤š AWQ åˆ¤æ–­ |
| `set_attention_type` | Qwen3 é¢å¤–å‘ `flash_attention_decode_gpu_pos_layer_` ä¼ æ’­ |
| **`attention_qkv` / `attention_qkv_with_graph` / `batched_attention_qkv`** | **æ ¸å¿ƒå·®å¼‚**ï¼šQwen2 æœ‰ Q/K/V biasï¼ŒQwen3 æœ‰ Q/K é€å¤´ RMSNorm + AWQ æ”¯æŒ |

**å”¯ä¸€çœŸæ­£ä¸åŒçš„åªæœ‰ Q/K/V æŠ•å½±ç›¸å…³çš„ 3 ä¸ªæ–¹æ³•**ï¼Œå…¶ä½™ 20 ä¸ªæ–¹æ³•å¯ä»¥å…±äº«ã€‚

### ä¸ºä»€ä¹ˆ Qwen3VL ä¸çº³å…¥ç»§æ‰¿ä½“ç³»

Qwen3VL ä¸ Qwen2/Qwen3 å­˜åœ¨æ ¹æœ¬æ€§çš„ API å·®å¼‚ï¼Œå¼ºè¡Œç»Ÿä¸€ä¼šå¼•å…¥è¿‡å¤šæ¡ä»¶åˆ†æ”¯ï¼š

- æ‰¹é‡çŸ©é˜µä¹˜ä½¿ç”¨ç›´æ¥ `cublasHgemm` è°ƒç”¨è€Œé layer æŠ½è±¡
- ä½¿ç”¨ M-RoPEï¼ˆå¤šç»´ä½ç½®ç¼–ç ï¼‰è€Œéæ ‡å‡† RoPE
- Flash Attention è°ƒç”¨æ–¹å¼ä¸åŒï¼ˆ`attention_qkv_with_graph` æ¥æ”¶ 2 ä¸ª pos tensorï¼‰
- æœ‰å®Œå…¨ç‹¬ç«‹çš„è§†è§‰ç¼–ç å™¨ï¼ˆViTï¼‰é€»è¾‘

---

## äºŒã€é‡æ„ç­–ç•¥ï¼šæ¨¡æ¿æ–¹æ³•æ¨¡å¼

é‡‡ç”¨ç»å…¸çš„ **æ¨¡æ¿æ–¹æ³•ï¼ˆTemplate Methodï¼‰** è®¾è®¡æ¨¡å¼ï¼š

1. **æå–å…¬å…±åŸºç±» `QwenBaseModel`**ï¼šå®ç°æ‰€æœ‰å…±äº«çš„æ¨ç†é€»è¾‘
2. **å°†å·®å¼‚ç‚¹å®šä¹‰ä¸ºçº¯è™šå‡½æ•°**ï¼š`attention_qkv()`, `attention_qkv_with_graph()`, `batched_attention_qkv()`
3. **é€šè¿‡å¤šæ€è®¿é—®å±‚æŒ‡é’ˆ**ï¼šå®šä¹‰ `QwenBaseLayers` åŸºç¡€ç»“æ„ä½“ + `get_base_layers()` çº¯è™šå‡½æ•°

### ç»§æ‰¿å…³ç³»å˜åŒ–

```
é‡æ„å‰:                              é‡æ„å:
Model                                Model
â”œâ”€â”€ Qwen2Model   (2104è¡Œ)            â”œâ”€â”€ QwenBaseModel (820è¡Œ) ğŸ†•
â”œâ”€â”€ Qwen3Model   (2095è¡Œ)            â”‚   â”œâ”€â”€ Qwen2Model  (1145è¡Œ) â¬‡-959
â””â”€â”€ Qwen3VLModel (3088è¡Œ)            â”‚   â””â”€â”€ Qwen3Model  (1262è¡Œ) â¬‡-833
                                     â””â”€â”€ Qwen3VLModel    (3088è¡Œ) ä¸å˜
```

---

## ä¸‰ã€å…·ä½“ä¿®æ”¹å†…å®¹

### 3.1 æ–°å¢ `kuiper/include/model/qwen_base.h`ï¼ˆ192è¡Œï¼‰

å®šä¹‰ä¸¤ä¸ªæ ¸å¿ƒæŠ½è±¡ï¼š

**`QwenBaseLayers` ç»“æ„ä½“**ï¼šæå–æ‰€æœ‰å…±äº«çš„ layer æŒ‡é’ˆ

```cpp
struct QwenBaseLayers {
  // éå‚æ•°å±‚ï¼ˆå…¨æ¨¡å‹å…±äº«å•å®ä¾‹ï¼‰
  std::shared_ptr<op::Layer> add_layer_, rope_layer_, swiglu_layer_, mha_layer_;

  // æ¯å±‚å‚æ•°æƒé‡å±‚
  std::vector<std::shared_ptr<op::Layer>> wq_layers_, wk_layers_, wv_layers_, wo_layers_;
  std::vector<std::shared_ptr<op::Layer>> w1_layers_, w2_layers_, w3_layers_, rmsnorm_layers_;
  std::shared_ptr<op::Layer> cls_layer_, embedding_layer_;

  // Flash Attention / KV Cache / Fused FFN / Batched å±‚...
  virtual ~QwenBaseLayers() = default;  // è™šææ„ï¼Œæ”¯æŒå¤šæ€
};
```

**`QwenBaseModel` ç±»**ï¼šå£°æ˜å…±äº«æ–¹æ³• + 3 ä¸ªçº¯è™šæ¥å£

```cpp
class QwenBaseModel : public Model {
 protected:
  // å­ç±»å¿…é¡»å®ç°ï¼ˆæ¨¡å‹å·®å¼‚ç‚¹ï¼‰
  virtual QwenBaseLayers* get_base_layers() const = 0;
  virtual void attention_qkv(...) const = 0;
  virtual void attention_qkv_with_graph(...) const = 0;
  virtual void batched_attention_qkv(...) const = 0;

  // å…±äº«å®ç°ï¼ˆ20ä¸ªæ–¹æ³•ï¼‰
  void attention_rms(...) const;
  void attention_mha(...) const;
  void attention_mha_with_graph(...) const;
  void feed_forward(...) const;
  void feed_forward_fused(...) const;
  void cls_logits(...) const;
  // ... ç­‰ç­‰

  std::shared_ptr<kernel::CudaConfig> cuda_config_;
  bool use_fused_ffn_ = true;
};
```

### 3.2 æ–°å¢ `kuiper/source/model/qwen_base.cpp`ï¼ˆ820è¡Œï¼‰

å®ç°äº† 20 ä¸ªå…±äº«æ–¹æ³•ã€‚å…³é”®è®¾è®¡å†³ç­–ï¼š

**AWQ å…¼å®¹æ€§å¤„ç†ï¼ˆè¶…é›†æ–¹æ¡ˆï¼‰**ï¼šåŸæ¥ Qwen2 çš„ `feed_forward_fused` ä¸æ£€æŸ¥ AWQï¼ŒQwen3 çš„ä¼šæ£€æŸ¥ã€‚ç»Ÿä¸€é‡‡ç”¨ Qwen3 çš„æ–¹å¼â€”â€”é€šè¿‡ `dynamic_pointer_cast<op::AWQMatmulLayer>` å°è¯•è½¬å‹ï¼š

```cpp
auto w1_awq = std::dynamic_pointer_cast<op::AWQMatmulLayer>(w1_layer);
if (w1_awq) {
    // AWQ è·¯å¾„ï¼ˆQwen3-AWQ èµ°è¿™é‡Œï¼‰
} else {
    // æ ‡å‡† MatmulLayer è·¯å¾„ï¼ˆQwen2 å’Œ Qwen3-FP16 èµ°è¿™é‡Œï¼‰
}
```

å¯¹ Qwen2 è€Œè¨€ï¼Œ`dynamic_pointer_cast` å§‹ç»ˆè¿”å› `nullptr`ï¼Œè‡ªç„¶èµ°æ ‡å‡†è·¯å¾„â€”â€”è¡Œä¸ºä¸åŸä»£ç å®Œå…¨ä¸€è‡´ï¼Œæ— é¢å¤–å¼€é”€ã€‚

**å±‚æŒ‡é’ˆè®¿é—®**é€šè¿‡ `get_base_layers()` è™šå‡½æ•°å®ç°å¤šæ€ï¼š

```cpp
void QwenBaseModel::attention_rms(int32_t layer_idx, const tensor::Tensor& input) const {
  auto* layers = get_base_layers();  // å¤šæ€è°ƒç”¨ï¼Œè¿”å› Qwen2Layers* æˆ– Qwen3Layers*
  // é€šè¿‡åŸºç±»æŒ‡é’ˆè®¿é—®å…±æœ‰æˆå‘˜
  layers->rmsnorm_layers_.at(layer_idx)->forward(input, rmsnorm_output);
}
```

### 3.3 ä¿®æ”¹ `kuiper/include/model/qwen2.h`ï¼ˆ165è¡Œ â†’ 48è¡Œï¼‰

- `#include "model.h"` â†’ `#include "qwen_base.h"`
- `class Qwen2Model : public Model` â†’ `class Qwen2Model : public QwenBaseModel`
- `struct Qwen2Layers { æ‰€æœ‰å±‚... }` â†’ `struct Qwen2Layers : public QwenBaseLayers { ä»…2ä¸ªç‰¹æœ‰å±‚ }`
- åˆ é™¤ 20 ä¸ªå…±äº«æ–¹æ³•å£°æ˜
- æ·»åŠ  `get_base_layers() override` å’Œ 3 ä¸ªçº¯è™šå‡½æ•°çš„ `override` å£°æ˜

`Qwen2Layers` åªä¿ç•™ Qwen2 ç‰¹æœ‰çš„å±‚ï¼š

```cpp
struct Qwen2Layers : public QwenBaseLayers {
  std::shared_ptr<op::BatchedMatmulLayer> batched_matmul_layer_;  // Qwen2ç‰¹æœ‰
  std::shared_ptr<op::BiasAddLayer> bias_add_layer_;               // Qwen2ç‰¹æœ‰ï¼ˆQ/K/V biasï¼‰
};
```

### 3.4 ä¿®æ”¹ `kuiper/include/model/qwen3.h`ï¼ˆ200è¡Œ â†’ 74è¡Œï¼‰

åŒç†ï¼Œ`Qwen3Layers` åªä¿ç•™ Qwen3 ç‰¹æœ‰çš„å±‚ï¼š

```cpp
struct Qwen3Layers : public QwenBaseLayers {
  std::shared_ptr<op::MRoPELayer> mrope_layer_;                     // M-RoPEï¼ˆVLç”¨ï¼‰
  std::shared_ptr<op::FlashAttentionDecodeGpuPosLayer> ...;         // GPU pos FA
  std::shared_ptr<op::RMSNormDimLayer> rmsnorm_dim_layer_;          // Q/K é€å¤´ RMSNorm
  // ... ç­‰
};
```

Qwen3 é¢å¤– override äº† `set_attention_type`ï¼Œå› ä¸ºå®ƒéœ€è¦å‘ `flash_attention_decode_gpu_pos_layer_` ä¹Ÿä¼ æ’­ attention typeï¼ˆåŸºç±»ç‰ˆæœ¬åªä¼ æ’­åˆ°åŸºç¡€çš„ FA å±‚ï¼‰ã€‚

### 3.5 ä¿®æ”¹ `kuiper/source/model/qwen2.cpp`ï¼ˆ2104è¡Œ â†’ 1145è¡Œï¼Œåˆ é™¤ 959 è¡Œï¼‰

- **åˆ é™¤** 20 ä¸ªå·²ç§»è‡³åŸºç±»çš„æ–¹æ³•å®ç°
- **ä¿ç•™** `Qwen2Layers::to_cuda`ã€æ„é€ å‡½æ•°ã€`init`ã€`init_mem`ã€`create_*_layers`ã€ä»¥åŠ 3 ä¸ª QKV æ–¹æ³•
- **ä¿®æ”¹** æ„é€ å‡½æ•°å§”æ‰˜ï¼š`Model(...)` â†’ `QwenBaseModel(...)`

### 3.6 ä¿®æ”¹ `kuiper/source/model/qwen3.cpp`ï¼ˆ2095è¡Œ â†’ 1262è¡Œï¼Œåˆ é™¤ 833 è¡Œï¼‰

åŒä¸Šå¤„ç†ã€‚é¢å¤–ä¿ç•™äº† Qwen3 ç‰¹æœ‰çš„ `set_attention_type` å’Œ `create_param_layers_awq`ã€‚

### 3.7 æœªä¿®æ”¹ `kuiper/source/model/qwen3_vl.cpp`ï¼ˆ3088è¡Œä¸å˜ï¼‰

å¦‚ä¸Šæ‰€è¿°ï¼ŒQwen3VL çš„ API å·®å¼‚è¿‡å¤§ï¼Œä¸çº³å…¥æ­¤ç»§æ‰¿ä½“ç³»ã€‚

---

## å››ã€é‡æ„åçš„å®Œæ•´ç±»ç»“æ„

```
Model (æŠ½è±¡åŸºç±»)
â”‚
â”œâ”€â”€ QwenBaseModel (æŠ½è±¡åŸºç±»ï¼Œæ–°å¢)
â”‚   â”‚
â”‚   â”‚  ã€å…±äº«æˆå‘˜ã€‘
â”‚   â”‚  #cuda_config_ : shared_ptr<CudaConfig>
â”‚   â”‚  #use_fused_ffn_ : bool
â”‚   â”‚
â”‚   â”‚  ã€å…±äº«æ–¹æ³•å®ç° (20ä¸ª)ã€‘
â”‚   â”‚  +forward(), +predict(), +embedding()
â”‚   â”‚  +prefill(), +decode()
â”‚   â”‚  +clear_kv_cache(), +set_attention_type()
â”‚   â”‚  #attention_rms(), #attention_mha(), #attention_mha_with_graph()
â”‚   â”‚  #feed_forward(), #feed_forward_fused()
â”‚   â”‚  #cls_logits(), #post_processing()
â”‚   â”‚  #batched_attention_rms() x2
â”‚   â”‚  #batched_attention_mha() x2
â”‚   â”‚  #batched_feed_forward(), #batched_feed_forward_optimized()
â”‚   â”‚
â”‚   â”‚  ã€çº¯è™šæ¥å£ (4ä¸ª)ã€‘
â”‚   â”‚  #get_base_layers() = 0
â”‚   â”‚  #attention_qkv() = 0
â”‚   â”‚  #attention_qkv_with_graph() = 0
â”‚   â”‚  #batched_attention_qkv() = 0
â”‚   â”‚
â”‚   â”œâ”€â”€ Qwen2Model
â”‚   â”‚   - qwen_layers_ : unique_ptr<Qwen2Layers>
â”‚   â”‚   + get_base_layers() â†’ qwen_layers_.get()
â”‚   â”‚   + attention_qkv()           ã€ç‰¹æœ‰ï¼šQ/K/V biasã€‘
â”‚   â”‚   + attention_qkv_with_graph() ã€ç‰¹æœ‰ï¼šQ/K/V bias + GPU posã€‘
â”‚   â”‚   + batched_attention_qkv()   ã€ç‰¹æœ‰ï¼šæ‰¹é‡ bias_addã€‘
â”‚   â”‚   + init(), init_mem(), create_*_layers()
â”‚   â”‚
â”‚   â””â”€â”€ Qwen3Model
â”‚       - qwen_layers_ : unique_ptr<Qwen3Layers>
â”‚       + get_base_layers() â†’ qwen_layers_.get()
â”‚       + attention_qkv()           ã€ç‰¹æœ‰ï¼šQ/K RMSNormã€‘
â”‚       + attention_qkv_with_graph() ã€ç‰¹æœ‰ï¼šQ/K RMSNorm + GPU posã€‘
â”‚       + batched_attention_qkv()   ã€ç‰¹æœ‰ï¼šAWQ + Q/K RMSNormã€‘
â”‚       + set_attention_type()      ã€ç‰¹æœ‰ï¼šä¼ æ’­åˆ° gpu_pos FA å±‚ã€‘
â”‚       + init(), init_mem(), create_*_layers(), create_param_layers_awq()
â”‚
â””â”€â”€ Qwen3VLModel (ç‹¬ç«‹ï¼Œæœªå˜æ›´)
    - ç›´æ¥ç»§æ‰¿ Modelï¼Œä¸çº³å…¥ QwenBaseModel ä½“ç³»


QwenBaseLayers (æ•°æ®ç»“æ„ä½“)
â”‚  æ‰€æœ‰å…±ç”¨ layer æŒ‡é’ˆï¼šadd, rope, swiglu, mha,
â”‚  wq/wk/wv/wo, w1/w2/w3, rmsnorm, cls, embedding,
â”‚  flash_attention_decode/prefill, kv_cache_key/value,
â”‚  fused_ffn, rope_gpu_pos, sin_cos_cache, mha_gpu_pos,
â”‚  batched_rope/add/swiglu/mha, batched_matmul_helper
â”‚
â”œâ”€â”€ Qwen2Layers
â”‚   + batched_matmul_layer_   (BatchedMatmulLayer)
â”‚   + bias_add_layer_         (BiasAddLayerï¼ŒQ/K/V bias ç”¨)
â”‚
â””â”€â”€ Qwen3Layers
    + mrope_layer_                        (M-RoPEï¼ŒVL ç”¨)
    + mrope_gpu_pos_layer_                (M-RoPE GPU pos)
    + batched_mrope_layer_                (æ‰¹é‡ M-RoPE)
    + fused_kv_cache_update_layer_        (èåˆ KV cache æ›´æ–°)
    + rmsnorm_dim_layer_                  (Q/K é€å¤´ RMSNorm)
    + copy_to_kv_cache_layer_             (KV cache æ‹·è´)
    + flash_attention_decode_gpu_pos_layer_ (GPU pos Flash Attention)
```

---

## äº”ã€é‡æ„åçš„æ¨ç†è°ƒç”¨æµç¨‹

### 5.1 Decode é˜¶æ®µï¼ˆé€ token ç”Ÿæˆï¼‰

```
åº”ç”¨å±‚ (inference_common.h) è°ƒç”¨:
  model.decode(input, pos, next)
    â””â”€â”€ QwenBaseModel::decode()              [qwen_base.cpp]
        â”‚
        â”œâ”€â”€ ã€CUDA Graph è·¯å¾„ã€‘
        â”‚   â”œâ”€â”€ å‡†å¤‡ GPU pos, decode_input buffer
        â”‚   â”œâ”€â”€ é¦–æ¬¡è°ƒç”¨æ—¶æ•è· Graph:
        â”‚   â”‚   â””â”€â”€ for layer_idx in 0..N-1:
        â”‚   â”‚       â”œâ”€â”€ attention_rms()           [åŸºç±»: RMSNorm â†’ rmsnorm_output]
        â”‚   â”‚       â”œâ”€â”€ attention_qkv_with_graph() â”€â”€â†’ ã€å¤šæ€åˆ†å‘åˆ°å­ç±»ã€‘
        â”‚   â”‚       â”‚   â”œâ”€â”€ Qwen2: WQÂ·x + bias, WKÂ·x + bias, WVÂ·x + bias, RoPE(GPU pos), KV cache
        â”‚   â”‚       â”‚   â””â”€â”€ Qwen3: WQÂ·x, WKÂ·x, WVÂ·x, Q/K RMSNorm, RoPE(GPU pos), KV cache
        â”‚   â”‚       â”œâ”€â”€ attention_mha_with_graph() [åŸºç±»: Flash Attention â†’ mha_output â†’ WOæŠ•å½±]
        â”‚   â”‚       â””â”€â”€ feed_forward_fused()       [åŸºç±»: æ®‹å·® + RMSNorm + Fused W1Â·W3Â·SwiGLU + W2 + æ®‹å·®]
        â”‚   â”œâ”€â”€ cls_logits()                       [åŸºç±»: æœ€ç»ˆRMSNorm + åˆ†ç±»å¤´]
        â”‚   â”œâ”€â”€ graph.launch()                     (åç»­è°ƒç”¨ç›´æ¥é‡æ”¾ Graph)
        â”‚   â””â”€â”€ argmax_sampler â†’ next token
        â”‚
        â””â”€â”€ ã€æ™®é€šè·¯å¾„ã€‘(æ—  Graph æˆ– Graph å¤±è´¥æ—¶)
            â”œâ”€â”€ for layer_idx in 0..N-1:
            â”‚   â”œâ”€â”€ attention_rms()
            â”‚   â”œâ”€â”€ attention_qkv()  â”€â”€â†’ ã€å¤šæ€åˆ†å‘åˆ°å­ç±»ã€‘
            â”‚   â”œâ”€â”€ attention_mha()
            â”‚   â””â”€â”€ feed_forward / feed_forward_fused
            â”œâ”€â”€ cls_logits()
            â””â”€â”€ post_processing() â†’ next token
```

### 5.2 Prefill é˜¶æ®µï¼ˆæ‰¹é‡å¤„ç† promptï¼‰

```
åº”ç”¨å±‚ (inference_common.h) è°ƒç”¨:
  model.prefill(embedding_output, seq_len, start_pos)
    â””â”€â”€ QwenBaseModel::prefill()             [qwen_base.cpp]
        â”‚
        â”œâ”€â”€ åˆ†é… double-buffer: hidden_buf0, hidden_buf1 (äº¤æ›¿ä½¿ç”¨ï¼Œé¿å…æ‹·è´)
        â”œâ”€â”€ åˆ†é… FFN buffer: ffn_norm, w1, w3, w2 (é¢„åˆ†é…å¤ç”¨ï¼Œé¿å…æ¯å±‚é‡åˆ†é…)
        â”‚
        â””â”€â”€ for layer_idx in 0..N-1:
            â”œâ”€â”€ ç¡®å®š layer_input / layer_output (double-buffer åˆ‡æ¢)
            â”œâ”€â”€ batched_attention_rms()          [åŸºç±»: æ‰¹é‡ RMSNorm â†’ rms_out]
            â”œâ”€â”€ batched_attention_qkv()           â”€â”€â†’ ã€å¤šæ€åˆ†å‘åˆ°å­ç±»ã€‘
            â”‚   â”œâ”€â”€ Qwen2: BatchedMatmul + bias_add + BatchedRoPE + KV cache memcpy
            â”‚   â””â”€â”€ Qwen3: BatchedMatmul/AWQ + Q/K RMSNorm + BatchedRoPE + KV cache memcpy
            â”œâ”€â”€ batched_attention_mha()          [åŸºç±»: FA prefill + WOæŠ•å½± (AWQå…¼å®¹)]
            â”œâ”€â”€ batched_add (æ®‹å·®è¿æ¥)            [åŸºç±»]
            â””â”€â”€ batched_feed_forward_optimized() [åŸºç±»: é¢„åˆ†é…bufferç‰ˆ FFN]
        â”‚
        â”œâ”€â”€ å–æœ€åä¸€ä¸ª token çš„ hidden state
        â””â”€â”€ cls_logits(last_hidden)              [åŸºç±»: æœ€ç»ˆRMSNorm + åˆ†ç±»å¤´]
```

### 5.3 å•æ­¥ Attention å†…éƒ¨æµç¨‹

```
æ¯ä¸€å±‚ Transformer Layer çš„æ‰§è¡Œæµç¨‹:

input â”€â”€â†’ [RMSNorm] â”€â”€â†’ rmsnorm_output
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“         â†“         â†“
                  [WQÂ·x]    [WKÂ·x]    [WVÂ·x]     â† å­ç±»å®ç° (attention_qkv)
                    â”‚         â”‚         â”‚
              â”Œâ”€â”€â”€â”€â”€â”¤   â”Œâ”€â”€â”€â”€â”€â”¤         â”‚
              â”‚ Qwen2: +bias  +bias     â”‚
              â”‚ Qwen3: Q-RMSNorm K-RMSNorm
              â”‚     â”‚         â”‚         â”‚
              â”‚   [RoPE]    [RoPE]      â”‚
              â”‚     â”‚         â”‚         â†“
              â”‚     â”‚         â””â”€â”€â†’ [KV Cache æ›´æ–°] â† å­ç±»å®ç°
              â”‚     â”‚         â”‚         â”‚
              â”‚     â†“         â†“         â†“         â† åŸºç±»å®ç° (attention_mha)
              â”‚   [Flash Attention / MHA] â”€â”€â†’ mha_output
              â”‚                                â”‚
              â”‚                              [WOÂ·x] â”€â”€â†’ attn_output
              â”‚                                              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [æ®‹å·® Add] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                   [FFN RMSNorm]              â† åŸºç±»å®ç° (feed_forward)
                                        â”‚
                                 â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                 â†“             â†“
                               [W1Â·x]       [W3Â·x]
                                 â”‚             â”‚
                                 â””â”€â”€â†’ [SwiGLU] â†â”˜
                                        â”‚
                                      [W2Â·x]
                                        â”‚
              input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [æ®‹å·® Add] â†â”€â”€â”˜
                                    â”‚
                                  output â†’ ä¸‹ä¸€å±‚çš„ input
```

---

## å…­ã€å…³é”®è®¾è®¡å†³ç­–

| å†³ç­– | åŸå›  |
|------|------|
| ç”¨ `dynamic_pointer_cast` åš AWQ åˆ¤æ–­ | Qwen2 æ—  AWQ å±‚ï¼Œcast è¿”å› nullptr èµ°æ ‡å‡†è·¯å¾„ï¼Œé›¶å¼€é”€å…¼å®¹ |
| `QwenBaseLayers` ç”¨è™šææ„ + è¿è¡Œæ—¶å¤šæ€ | è¿è¡Œæ—¶å¤šæ€è¶³å¤Ÿï¼Œæ— éœ€ CRTP ç¼–è¯‘æœŸå¤šæ€ï¼›ç®€æ´æ˜“ç»´æŠ¤ |
| Qwen3VL ä¸çº³å…¥ç»§æ‰¿ | API å·®å¼‚å¤ªå¤§ï¼ˆç›´æ¥ cublasã€M-RoPEã€ViTï¼‰ï¼Œå¼ºè¡Œç»Ÿä¸€ä¼šå¼•å…¥è¿‡å¤šæ¡ä»¶åˆ†æ”¯ |
| `cuda_config_` æ”¾åœ¨åŸºç±» | ä¸¤ä¸ªå­ç±»éƒ½éœ€è¦ï¼Œé¿å…é‡å¤å£°æ˜ |
| `set_attention_type` å…è®¸ Qwen3 å† override | Qwen3 éœ€è¦é¢å¤–ä¼ æ’­åˆ° `flash_attention_decode_gpu_pos_layer_`ï¼Œè°ƒç”¨é“¾ä¸º `Qwen3::set_attention_type` â†’ `QwenBaseModel::set_attention_type` â†’ `Model::set_attention_type` |
| `forward()` ä¸­ä½¿ç”¨ `use_fused_ffn_` æ ‡å¿— | åŸºç±»ç»Ÿä¸€æ§åˆ¶æ˜¯å¦ä½¿ç”¨ Fused FFN å†…æ ¸ï¼Œå­ç±»æ— éœ€å…³å¿ƒ |

---

## ä¸ƒã€ä»£ç é‡å˜åŒ–æ±‡æ€»

| æ–‡ä»¶ | é‡æ„å‰ | é‡æ„å | å˜åŒ– |
|------|--------|--------|------|
| `qwen_base.h` | 0 | 192 | +192 |
| `qwen_base.cpp` | 0 | 820 | +820 |
| `qwen2.h` | 165 | 48 | -117 |
| `qwen2.cpp` | 2,104 | 1,145 | **-959** |
| `qwen3.h` | 200 | 74 | -126 |
| `qwen3.cpp` | 2,095 | 1,262 | **-833** |
| `qwen3_vl.h` | 511 | 511 | 0 |
| `qwen3_vl.cpp` | 3,088 | 3,088 | 0 |
| **æ€»è®¡** | **8,163** | **7,140** | **-1,023** |

å‡€å‡å°‘ **1,023 è¡Œä»£ç **ï¼Œæ¶ˆé™¤äº† 20 ä¸ªæ–¹æ³•åœ¨ä¸¤ä¸ªæ–‡ä»¶ä¸­çš„é‡å¤å®ç°ã€‚

---

## å…«ã€æµ‹è¯•éªŒè¯

æ‰€æœ‰ 5 ä¸ªæ¨ç†åœºæ™¯å‡é€šè¿‡æµ‹è¯•ï¼Œè¾“å‡ºå†…å®¹ä¸é‡æ„å‰ä¸€è‡´ï¼š

| # | æµ‹è¯•å‘½ä»¤ | Prefill | Decode | ç»“æœ |
|---|---------|---------|--------|------|
| 1 | `qwen3_infer` Qwen3-8B-fp16 | 131 tok/s | 10.3 tok/s | âœ… é€šè¿‡ |
| 2 | `qwen3_infer` Qwen3-8B-awq | 158 tok/s | 10.2 tok/s | âœ… é€šè¿‡ |
| 3 | `qwen_infer` Qwen2.5-7B (FP32) | 6.1 tok/s | 5.7 tok/s | âœ… é€šè¿‡ |
| 4 | `qwen_infer` Qwen2.5-7B-fp16 | 150 tok/s | 11.0 tok/s | âœ… é€šè¿‡ |
| 5 | `qwen3_vl_infer` Qwen3-VL-8B-fp16 | 499 tok/s | 9.8 tok/s | âœ… é€šè¿‡ |

æ€§èƒ½æ•°æ®ä¸é‡æ„å‰å®Œå…¨ä¸€è‡´ï¼Œé‡æ„ä»…æ”¹å˜ä»£ç ç»„ç»‡ç»“æ„ï¼Œä¸å½±å“è¿è¡Œæ—¶è¡Œä¸ºå’Œæ€§èƒ½ã€‚

---

## ä¹ã€åç»­ç»´æŠ¤æ”¶ç›Š

é‡æ„åï¼Œä»¥ä¸‹å¸¸è§ä¿®æ”¹åœºæ™¯åªéœ€æ”¹åŠ¨ **ä¸€å¤„** è€ŒéåŸæ¥çš„ **ä¸¤å¤„**ï¼š

- ä¿®æ”¹ decode/prefill å¾ªç¯é€»è¾‘ â†’ æ”¹ `qwen_base.cpp`
- ä¿®æ”¹ Flash Attention è°ƒç”¨æ–¹å¼ â†’ æ”¹ `qwen_base.cpp`
- ä¿®æ”¹ CUDA Graph æ•è·/é‡æ”¾ç­–ç•¥ â†’ æ”¹ `qwen_base.cpp`
- ä¿®æ”¹ KV cache ç®¡ç†é€»è¾‘ â†’ æ”¹ `qwen_base.cpp`
- ä¿®æ”¹ Fused FFN å†…æ ¸è°ƒç”¨ â†’ æ”¹ `qwen_base.cpp`
- ä¿®æ”¹é‡‡æ ·/åå¤„ç†é€»è¾‘ â†’ æ”¹ `qwen_base.cpp`
- æ·»åŠ æ–°çš„å…±äº«ä¼˜åŒ–ï¼ˆå¦‚ Attention èåˆï¼‰â†’ æ”¹ `qwen_base.cpp`ï¼Œå­ç±»æ— éœ€å˜åŠ¨

å¦‚éœ€æ·»åŠ æ–°çš„ Qwen å˜ä½“ï¼ˆå¦‚ Qwen4ï¼‰ï¼Œåªéœ€ç»§æ‰¿ `QwenBaseModel` å¹¶å®ç° 4 ä¸ªçº¯è™šå‡½æ•°å³å¯è·å¾—å®Œæ•´çš„æ¨ç†èƒ½åŠ›ã€‚
