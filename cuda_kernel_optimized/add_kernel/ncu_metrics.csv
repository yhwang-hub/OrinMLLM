"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","87.54",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","15.57",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L2 in the Memory Workload Analysis section.","",""
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","92.95"
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.54 active warps per scheduler, but only an average of 0.22 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","12.46"
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 49.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 82.9% of the total average of 59.9 cycles between issuing two instructions.","global","12.46"
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 65536 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","3.525"
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Block Size","","512",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Grid Size","","4,096",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","16",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","38.58",
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (80.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.46"
"0","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(512, 1, 1)","(4096, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","86.31",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","17.00",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L2 in the Memory Workload Analysis section.","",""
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","91.23"
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.98 active warps per scheduler, but only an average of 0.22 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","13.69"
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 47.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 83.7% of the total average of 56.3 cycles between issuing two instructions.","global","13.69"
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Block Size","","256",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Grid Size","","4,096",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","16",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","40.39",
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.69"
"1","3637839","bench_add","127.0.0.1","original::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(4096, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","41.53",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","52.00",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (33.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.48 active warps per scheduler, but only an average of 0.87 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","46.5"
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 9.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 51.1% of the total average of 17.7 cycles between issuing two instructions.","global","46.5"
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Block Size","","256",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Grid Size","","8,192",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","16",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","38.84",
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (80.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","19.08"
"2","3637839","bench_add","127.0.0.1","original::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","61.50",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","24.51",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","OPT","Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L2 bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or whether there are values you can (re)compute.","",""
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","87.37"
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.24 active warps per scheduler, but only an average of 0.31 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","38.5"
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.0% of the total average of 36.5 cycles between issuing two instructions.","global","38.5"
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Block Size","","256",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Grid Size","","8,192",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","16",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","38.02",
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","20.8"
"3","3637839","bench_add","127.0.0.1","original::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(8192, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","88.60",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","5.09",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L2 in the Memory Workload Analysis section.","",""
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","97.1"
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 10.86 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","11.4"
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 193.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 92.4% of the total average of 209.4 cycles between issuing two instructions.","global","11.4"
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 65536 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","1.452"
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","Launch Statistics","Block Size","","256",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","Launch Statistics","Grid Size","","2,048",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","22",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","43.58",
"4","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp32(int, const float *, const float *, float *)","1","7","(256, 1, 1)","(2048, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","86.63",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","5.17",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L2 in the Memory Workload Analysis section.","",""
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","96.85"
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 10.68 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","13.37"
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 185.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 91.7% of the total average of 202.8 cycles between issuing two instructions.","global","13.37"
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Block Size","","256",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Grid Size","","1,024",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","16",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","42.88",
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.68"
"5","3637839","bench_add","127.0.0.1","optimized::add_kernel_cu_fp16_impl(int, const __half *, const __half *, __half *)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","83.57",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.05",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L2 in the Memory Workload Analysis section.","",""
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","95.26"
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 10.28 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","16.43"
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 83.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 78.9% of the total average of 106.2 cycles between issuing two instructions.","global","16.43"
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","Launch Statistics","Block Size","","256",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","Launch Statistics","Grid Size","","1,024",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","32",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","40.93",
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.74"
"6","3637839","bench_add","127.0.0.1","optimized::broadcast_add_bias_fp16_kernel(const __half *, const __half *, __half *, int, int)","1","7","(256, 1, 1)","(2, 512, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","GPU Speed Of Light Throughput","L2 Cache Throughput","%","86.41",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","5.17",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L2 in the Memory Workload Analysis section.","",""
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","96.87"
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","MemoryWorkloadAnalysis_Chart","","","","MemoryL2Compression","WRN","The optional metric dram__bytes_read.sum.pct_of_peak_sustained_elapsed could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 10.65 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","13.59"
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 186.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 92.0% of the total average of 202.8 cycles between issuing two instructions.","global","13.59"
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Block Size","","256",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Grid Size","","1,024",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Registers Per Thread","register/thread","16",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Occupancy","Achieved Active Warps Per SM","warp","42.75",
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.93"
"7","3637839","bench_add","127.0.0.1","optimized::add_vec_fp16_kernel(__half *, const __half *, __half *, int)","1","7","(256, 1, 1)","(1024, 1, 1)","0","8.7","WorkloadDistribution","","","","WorkloadImbalance","WRN","The optional metric dram__cycles_active.avg could not be found. Collecting it as an additional metric could enable the rule to provide more guidance.","",""
